{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TypeWriter\n",
    "This notebook runs the TypeWriter method for predicting types of Python methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter import config_TW\n",
    "from os.path import join, exists, isdir\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import result_proc\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from result_proc import copy_results, clean_output\n",
    "# Copying the results and cleaning the output of last run\n",
    "#copy_results('./output/reports/', './results/')\n",
    "\n",
    "# Delete all the files in the output folder\n",
    "#clean_output('./output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Python projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gh_query import load_json, gen_json_file, find_current_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only select Python projects that has `mypy` as one of its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = load_json('./data/mypy-dependents-by-stars.json')\n",
    " \n",
    "gen_json_file('./data/py_projects_all.json', repos, find_current_repos('./data/paper-dataset/Repos', True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads selected repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = load_json('./data/py_projects_all.json')\n",
    "print(\"number of projects:\", len(repos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output folder and dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a name to output directory. It'll be created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_EMBEDDINGS_DIRECTORY = join(OUTPUT_DIR, 'embed')\n",
    "OUTPUT_DIRECTORY_TW = join(OUTPUT_DIR, 'funcs')\n",
    "AVAILABLE_TYPES_DIR = join(OUTPUT_DIR, 'avl_types')\n",
    "RESULTS_DIR = join(OUTPUT_DIR, \"results\")\n",
    "\n",
    "ML_INPUTS_PATH_TW = join(OUTPUT_DIR, \"ml_inputs\")\n",
    "ML_PARAM_TW_TRAIN = join(ML_INPUTS_PATH_TW, \"_ml_param_train.csv\")\n",
    "ML_PARAM_TW_TEST = join(ML_INPUTS_PATH_TW, \"_ml_param_test.csv\")\n",
    "ML_RET_TW_TRAIN = join(ML_INPUTS_PATH_TW, \"_ml_ret_train.csv\")\n",
    "ML_RET_TW_TEST = join(ML_INPUTS_PATH_TW, \"_ml_ret_test.csv\")\n",
    "\n",
    "VECTOR_OUTPUT_DIR_TW = join(OUTPUT_DIR, 'vectors')\n",
    "VECTOR_OUTPUT_TRAIN = join(VECTOR_OUTPUT_DIR_TW, \"train\")\n",
    "VECTOR_OUTPUT_TEST = join(VECTOR_OUTPUT_DIR_TW, \"test\")\n",
    "\n",
    "W2V_MODEL_TOKEN_DIR = join(OUTPUT_EMBEDDINGS_DIRECTORY, 'w2v_token_model.bin')\n",
    "W2V_MODEL_COMMENTS_DIR = join(OUTPUT_EMBEDDINGS_DIRECTORY, 'w2v_comments_model.bin')\n",
    "\n",
    "DATA_FILE_TW = join(ML_INPUTS_PATH_TW, \"_all_data.csv\")\n",
    "\n",
    "LABEL_ENCODER_PATH_TW = join(ML_INPUTS_PATH_TW, \"label_encoder.pkl\")\n",
    "TYPES_FILE_TW = join(ML_INPUTS_PATH_TW, \"_most_frequent_types.csv\")\n",
    "\n",
    "dirs = [OUTPUT_EMBEDDINGS_DIRECTORY, OUTPUT_DIRECTORY_TW, AVAILABLE_TYPES_DIR, RESULTS_DIR,\n",
    "        ML_INPUTS_PATH_TW, VECTOR_OUTPUT_DIR_TW, VECTOR_OUTPUT_TRAIN, VECTOR_OUTPUT_TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "config_TW.create_dirs(dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extracting functions\n",
    "Extract natural language information and preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltpy.preprocessing.pipeline import Pipeline\n",
    "p = Pipeline('./data/paper-dataset/Repos/', OUTPUT_DIRECTORY_TW, AVAILABLE_TYPES_DIR)\n",
    "p.run_pipeline_manual(repos, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates dataframe or loads an existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltpy.input_preparation.generate_df import list_files, parse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_TW.CACHE_TW and os.path.exists(DATA_FILE_TW):\n",
    "    print(\"Loading cached copy\")\n",
    "    df = pd.read_csv(DATA_FILE_TW)\n",
    "else:\n",
    "    DATA_FILES = list_files(OUTPUT_DIRECTORY_TW)\n",
    "    print(\"Found %d datafiles\" % len(DATA_FILES))\n",
    "    #print(DATA_FILES)\n",
    "    df = parse_df(DATA_FILES, batch_size=4098)\n",
    "    print(\"Dataframe loaded writing it to CSV\")\n",
    "    df.to_csv(DATA_FILE_TW, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial dataset before processing parameter and returns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['arg_names'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats of the repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of source files: \", len(df.file.unique()))\n",
    "print(\"Number of functions: \", len(df.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of functions with comments: \",\n",
    "      df[(~df['return_descr'].isnull()) | (~df['func_descr'].isnull())].shape[0])\n",
    "print(\"Number of functions with return types: \", df['return_type'].count())\n",
    "print(\"Number of functions with both: \",\n",
    "      df[((~df['return_descr'].isnull()) | (~df['func_descr'].isnull())) & (~df['return_type'].isnull())].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the intial dataset based on source files. Later on, we use this to split the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_files, test_files = train_test_split(pd.DataFrame(df['file'].unique(), columns=['file']),\n",
    "                                           test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['file'].isin(train_files.to_numpy().flatten())]\n",
    "print(\"Number of functions in train set: \", df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['file'].isin(test_files.to_numpy().flatten())]\n",
    "print(\"Number of functions in test set: \", df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter import prepocessing\n",
    "reload(prepocessing)\n",
    "from typewriter.prepocessing import filter_functions, gen_argument_df_TW, gen_most_frequent_avl_types, \\\n",
    "                                    encode_aval_types_TW\n",
    "from dltpy.input_preparation.generate_df import filter_return_dp, format_df, encode_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters trivial functions such as `__str__` and `__len__` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_functions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts informations for functions' arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = gen_argument_df_TW(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore `self` arguments and args with type of `Any` or `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_count = df_params['arg_name'].count()\n",
    "args_with_annot = df_params[df_params['arg_type'] != ''].shape[0]\n",
    "print(\"Number of arguments: \", args_count)\n",
    "print(\"Args with type annotations: \", args_with_annot)\n",
    "df_params = df_params[(df_params['arg_name'] != 'self') & ((df_params['arg_type'] != 'Any') & \\\n",
    "                                                          (df_params['arg_type'] != 'None'))]\n",
    "print(\"Ignored trivial types: \", (args_count - df_params.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore arguments without a type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = df_params[df_params['arg_type'] != '']\n",
    "print(\"Number of arguments with types: \", df_params.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters out functions:\n",
    "- without a return type\n",
    "- with the return type of `Any` or `None`\n",
    "- without a return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_return_dp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['arg_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = format_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode types as int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_params, label_encoder, uniq_types = encode_types(df, df_params, TYPES_FILE_TW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add argument names as a string except self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['arg_names_str'] = df['arg_names'].apply(lambda l: \" \".join([v for v in l if v != 'self']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add return expressions as a string, replace self. and self within expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['return_expr_str'] = df['return_expr'].apply(lambda l: \" \".join([re.sub(r\"self\\.?\", '', v) for v in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all columns useless for the ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['author', 'repo', 'has_type', 'arg_names', 'arg_types', 'arg_descrs', 'return_expr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts top 1000 available types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_TW.CACHE_TW and os.path.exists(os.path.join(AVAILABLE_TYPES_DIR,\n",
    "                                         'top_%d_types.csv' % (config_TW.AVAILABLE_TYPES_NUMBER-1))):\n",
    "    df_types = pd.read_csv(os.path.join(AVAILABLE_TYPES_DIR,\n",
    "                                         'top_%d_types.csv' % (config_TW.AVAILABLE_TYPES_NUMBER-1)))\n",
    "else:\n",
    "    df_types = gen_most_frequent_avl_types(AVAILABLE_TYPES_DIR, config_TW.AVAILABLE_TYPES_NUMBER-1,\n",
    "                                           True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params, df = encode_aval_types_TW(df_params, df, df_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ret_aval_enc'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the datapoints and types coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_enc_types = np.concatenate((df_params['arg_type_enc'].values, df['return_type_enc'].values))\n",
    "other_type_count = np.count_nonzero(all_enc_types == label_encoder.transform(['other'])[0])\n",
    "print(\"Number of datapoints with other types: \", other_type_count)\n",
    "print(\"The percentage of covered unique types: %.2f%%\" % \\\n",
    "      ((config_TW.AVAILABLE_TYPES_NUMBER / len(uniq_types))*100))\n",
    "print(\"The percentage of all datapoints covered by considered types: %.2f%%\" %\\\n",
    "      ((1 - other_type_count / all_enc_types.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final arguments data before embedding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final return data before embedding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits parameters and returns type dataset by file into a train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params_train = df_params[df_params['file'].isin(train_files.to_numpy().flatten())]\n",
    "df_params_test = df_params[df_params['file'].isin(test_files.to_numpy().flatten())]\n",
    "print(df_params_train.shape, df_params_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret_train = df[df['file'].isin(train_files.to_numpy().flatten())]\n",
    "df_ret_test = df[df['file'].isin(test_files.to_numpy().flatten())]\n",
    "print(df_ret_train.shape, df_ret_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that there is no overlap between the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_params_train['file'].tolist()).intersection(set(df_params_test['file'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_ret_train['file'].tolist()).intersection(set(df_ret_test['file'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the dataframes and the label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ML_INPUTS_PATH_TW):\n",
    "    os.makedirs(ML_INPUTS_PATH_TW)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH_TW, 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)\n",
    "    \n",
    "#df.to_csv(config.ML_RETURN_DF_PATH_TW, index=False)\n",
    "#df_params.to_csv(config.ML_PARAM_DF_PATH_TW, index=False)\n",
    "df_params_train.to_csv(ML_PARAM_TW_TRAIN, index=False)\n",
    "df_params_test.to_csv(ML_PARAM_TW_TEST, index=False)\n",
    "df_ret_train.to_csv(ML_RET_TW_TRAIN, index=False)\n",
    "df_ret_test.to_csv(ML_RET_TW_TEST, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots 20 most frequent types in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proc.plot_top_n_types(TYPES_FILE_TW, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter import extraction\n",
    "from typewriter.extraction import EmbeddingTypeWriter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "reload(extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads dataset for parametes and return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.read_csv(ML_PARAM_TW_TRAIN)\n",
    "return_df = pd.read_csv(ML_RET_TW_TRAIN)\n",
    "\n",
    "print(\"Number of parameters types:\", param_df.shape[0])\n",
    "print(\"Number of returns types\", return_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = EmbeddingTypeWriter(param_df, return_df, W2V_MODEL_COMMENTS_DIR,\n",
    "                               W2V_MODEL_TOKEN_DIR)\n",
    "embedder.train_token_model()\n",
    "embedder.train_comment_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads pre-trained W2V models for TypeWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_token_model = Word2Vec.load(W2V_MODEL_TOKEN_DIR)\n",
    "w2v_comments_model = Word2Vec.load(W2V_MODEL_COMMENTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats of the W2V models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W2V statistics: \")\n",
    "print(\"W2V token model total amount of words : \" + str(w2v_token_model.corpus_total_words))\n",
    "print(\"W2V comments model total amount of words : \" + str(w2v_comments_model.corpus_total_words))\n",
    "print(\"\\n Top 20 words for token model:\")\n",
    "print(w2v_token_model.wv.index_to_key[:20])\n",
    "print(\"\\n Top 20 words for comments model:\")\n",
    "print(w2v_comments_model.wv.index_to_key[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter.extraction import IdentifierSequence, TokenSequence, CommentSequence, \\\n",
    "                                  process_datapoints_TW, gen_aval_types_datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process parameter datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_trans_func_param = lambda row: IdentifierSequence(w2v_token_model, row.arg_name, row.other_args,\n",
    "                                                     row.func_name)\n",
    "token_trans_func_param = lambda row: TokenSequence(w2v_token_model, 7, 3, row.arg_occur, None)\n",
    "\n",
    "cm_trans_func_param = lambda row: CommentSequence(w2v_comments_model, row.func_descr, row.arg_comment, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_param_X_train = process_datapoints_TW(ML_PARAM_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                             'identifiers_', 'param_train', id_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_param_X_test = process_datapoints_TW(ML_PARAM_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                            'identifiers_', 'param_test', id_trans_func_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_param_X_train = process_datapoints_TW(ML_PARAM_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                                'tokens_', 'param_train', token_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_param_X_test = process_datapoints_TW(ML_PARAM_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                                'tokens_', 'param_test', token_trans_func_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_param_X_train = process_datapoints_TW(ML_PARAM_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                            'comments_', 'param_train', cm_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_param_X_test = process_datapoints_TW(ML_PARAM_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                            'comments_', 'param_test', cm_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifiers' train set parameters: \", dp_ids_param_X_train.shape)\n",
    "print(\"Tokens' train set parameters: \", dp_tokens_param_X_train.shape)\n",
    "print(\"Comments' train parameters: \", dp_cms_param_X_train.shape)\n",
    "print(\"Identifiers' test set parameters: \", dp_ids_param_X_test.shape)\n",
    "print(\"Tokens' test set parameters: \", dp_tokens_param_X_test.shape)\n",
    "print(\"Comments' test set parameters: \", dp_cms_param_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process returns datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_trans_func_ret = lambda row: IdentifierSequence(w2v_token_model, None, row.arg_names_str, row.name)\n",
    "\n",
    "token_trans_func_ret = lambda row: TokenSequence(w2v_token_model, 7, 3, None, row.return_expr_str)\n",
    "\n",
    "cm_trans_func_ret = lambda row: CommentSequence(w2v_comments_model, row.func_descr, None, row.return_descr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_ret_X_train = process_datapoints_TW(ML_RET_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                                     'identifiers_', 'ret_train', id_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_ret_X_test = process_datapoints_TW(ML_RET_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                                    'identifiers_', 'ret_test', id_trans_func_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_ret_X_train = process_datapoints_TW(ML_RET_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                                        'tokens_', 'ret_train', token_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_ret_X_test = process_datapoints_TW(ML_RET_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                                        'tokens_', 'ret_test', token_trans_func_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_ret_X_train = process_datapoints_TW(ML_RET_TW_TRAIN, VECTOR_OUTPUT_TRAIN, \n",
    "                                                     'comments_', 'ret_train', cm_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_ret_X_test = process_datapoints_TW(ML_RET_TW_TEST, VECTOR_OUTPUT_TEST, \n",
    "                                                     'comments_', 'ret_test', cm_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifiers' train set return: \", dp_ids_ret_X_train.shape)\n",
    "print(\"Identifiers' test set return: \", dp_ids_ret_X_test.shape)\n",
    "print(\"Tokens' train set return: \", dp_tokens_ret_X_train.shape)\n",
    "print(\"Tokens' test set return: \", dp_tokens_ret_X_test.shape)\n",
    "print(\"Comments' train set return:\" , dp_cms_ret_X_train.shape)\n",
    "print(\"Comments' test set return:\" , dp_cms_ret_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates datapoints for available types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_params_train_aval_types, dp_ret_train_aval_types = gen_aval_types_datapoints(ML_PARAM_TW_TRAIN,\n",
    "                                                                                ML_RET_TW_TRAIN,\n",
    "                                                                               'train',\n",
    "                                                                                VECTOR_OUTPUT_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_params_test_aval_types, dp_ret_test_aval_types = gen_aval_types_datapoints(ML_PARAM_TW_TEST,\n",
    "                                                                              ML_RET_TW_TEST,\n",
    "                                                                              'test',\n",
    "                                                                              VECTOR_OUTPUT_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available types-parameters-train:\", dp_params_train_aval_types.shape)\n",
    "print(\"Available types-returns-train:\", dp_ret_train_aval_types.shape)\n",
    "print(\"Available types-parameters-test:\", dp_params_test_aval_types.shape)\n",
    "print(\"Available types-returns-test:\", dp_ret_test_aval_types.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates type vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltpy.input_preparation.df_to_vec import generate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_y_train, ret_y_train = generate_labels(ML_PARAM_TW_TRAIN, ML_RET_TW_TRAIN,\n",
    "                                              'train', VECTOR_OUTPUT_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_y_test, ret_y_test = generate_labels(ML_PARAM_TW_TEST, ML_RET_TW_TEST,\n",
    "                                            'test', VECTOR_OUTPUT_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Learning the neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter.model import load_data_tensors_TW, TWModel, train_loop_TW, \\\n",
    "                             evaluate_TW, report_TW, load_label_tensors_TW, \\\n",
    "                             TWModelA, EnhancedTWModel, BaseLineModel\n",
    "from statistics import mean\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"-- Using {device} for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads parameters' data vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_param_train_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_param_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_param_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_param_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_datapoints_y.npy'))\n",
    "\n",
    "def load_param_test_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_param_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_param_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_param_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_datapoints_y.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads return' data vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ret_train_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_ret_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_ret_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_ret_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_datapoints_y.npy'))\n",
    "\n",
    "def load_ret_test_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_ret_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_ret_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_ret_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_datapoints_y.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatanates parameters and return data vectors for combined prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_train_data():\n",
    "    return torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_param_train_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_ret_train_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_param_train_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_ret_train_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_param_train_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_ret_train_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_aval_types_dp.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_aval_types_dp.npy')))), \\\n",
    "           torch.cat((load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_datapoints_y.npy')),\n",
    "                      load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_datapoints_y.npy'))))\n",
    "\n",
    "def load_combined_test_data():\n",
    "    return torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_param_test_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_ret_test_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_param_test_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_ret_test_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_param_test_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_ret_test_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_aval_types_dp.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_aval_types_dp.npy')))), \\\n",
    "           torch.cat((load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_datapoints_y.npy')),\n",
    "                      load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_datapoints_y.npy'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train = {'combined': load_combined_train_data,\n",
    "                  'return': load_ret_train_data,\n",
    "                  'argument': load_param_train_data}\n",
    "datasets_test = {'combined': load_combined_test_data,\n",
    "                 'return': load_ret_test_data,\n",
    "                 'argument': load_param_test_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = config_TW.W2V_VEC_LENGTH\n",
    "hidden_size = 768\n",
    "output_size = 1000\n",
    "num_layers = 1\n",
    "learning_rate = 0.002\n",
    "dropout_rate = 0.25\n",
    "epochs = 25\n",
    "top_n_pred = [1, 3, 5]\n",
    "n_rep = 1\n",
    "batch_size = 2048\n",
    "train_split_size = 0.8\n",
    "data_loader_workers = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete neural model of TypeWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TWModel(input_size, hidden_size, config_TW.AVAILABLE_TYPES_NUMBER, num_layers,\n",
    "                output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural model of TypeWriter without available types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TWModelA(input_size, hidden_size, num_layers, output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete neurel model of TypeWriter with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedTWModel(input_size, hidden_size, config_TW.AVAILABLE_TYPES_NUMBER, num_layers,\n",
    "                output_size, dropout_rate).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data parallesim for mutli-GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_other = pickle.load(open(LABEL_ENCODER_PATH_TW, 'rb')).transform(['other'])[0]\n",
    "\n",
    "for d in datasets_train:\n",
    "    print(f\"Loading {d} data for model {model.module.__class__.__name__}\")\n",
    "    #X_id, X_tok, X_cm, X_type, Y = datasets[d]\n",
    "    load_data_t = time.time()\n",
    "    X_id_train, X_tok_train, X_cm_train, X_type_train, Y_train = datasets_train[d]()\n",
    "    X_id_test, X_tok_test, X_cm_test, X_type_test, Y_test = datasets_test[d]()\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_id_train, X_tok_train, X_cm_train, X_type_train,\n",
    "                                            Y_train), batch_size=batch_size, shuffle=True,\n",
    "                                            pin_memory=True, num_workers=data_loader_workers)\n",
    "    \n",
    "    test_loader = DataLoader(TensorDataset(X_id_test, X_tok_test, X_cm_test, X_type_test,\n",
    "                                           Y_test), batch_size=batch_size)\n",
    "    print(\"Loaded train and test sets in %.2f min\" % ((time.time()-load_data_t) / 60))\n",
    "    \n",
    "    for i in range(1, n_rep+1):\n",
    "        \n",
    "        train_t = time.time()\n",
    "        train_loop_TW(model, train_loader, learning_rate, epochs)\n",
    "        print(\"Training finished in %.2f min\" % ((time.time()-train_t) / 60))\n",
    "        eval_t = time.time()\n",
    "        y_true, y_pred = evaluate_TW(model, test_loader, top_n=max(top_n_pred))\n",
    "        print(\"Prediction finished in %.2f min\" % ((time.time()-eval_t) / 60))\n",
    "        \n",
    "        # Ignore other type\n",
    "        idx = (y_true != idx_of_other) & (y_pred[:, 0] != idx_of_other)\n",
    "        f1_score_top_n = []\n",
    "        for top_n in top_n_pred:\n",
    "            filename = f\"{model.module.__class__.__name__}_{d}_{i}_{top_n}\"\n",
    "            report_TW(y_true, y_pred, top_n, f\"{filename}_unfiltered\", RESULTS_DIR)\n",
    "            report = report_TW(y_true[idx], y_pred[idx], top_n, f\"{filename}_filtered\", RESULTS_DIR)\n",
    "            f1_score_top_n.append(report['macro avg']['f1-score'])\n",
    "        print(\"Mean f1_score:\", mean(f1_score_top_n))\n",
    "        \n",
    "        model.module.reset_model_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_other = pickle.load(open(LABEL_ENCODER_PATH_TW, 'rb')).transform(['other'])[0]\n",
    "\n",
    "baseline_model = BaseLineModel(TYPES_FILE_TW)\n",
    "\n",
    "for d in datasets_train:\n",
    "    print(f\"Loading {d} data for model {baseline_model.__class__.__name__}\")\n",
    "   \n",
    "    X_id_test, X_tok_test, X_cm_test, X_type_test, y_test = datasets_test[d]()\n",
    "    y_test = y_test.numpy()\n",
    "    \n",
    "    y_pred = baseline_model.predict(X_id_test)\n",
    "    \n",
    "    # Ignore other type\n",
    "    idx = (y_test != idx_of_other) & (y_pred[:, 0] != idx_of_other)\n",
    "    f1_score_top_n = []\n",
    "    for top_n in top_n_pred:\n",
    "        filename = f\"{baseline_model.__class__.__name__}_{d}_{1}_{top_n}\"\n",
    "        report_TW(y_test, y_pred, top_n, f\"{filename}_unfiltered\", RESULTS_DIR)\n",
    "        report = report_TW(y_test[idx], y_pred[idx], top_n, f\"{filename}_filtered\", RESULTS_DIR)\n",
    "        print(report['weighted avg'])\n",
    "        f1_score_top_n.append(report['weighted avg']['f1-score'])\n",
    "    print(\"Mean f1_score:\", mean(f1_score_top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import result_proc\n",
    "reload(result_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = result_proc.eval_result(RESULTS_DIR, 'EnhancedTWModel', 'return', 'filtered', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proc.plot_result(res, \"NaiveBaseline-Return-MacroAvg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proc.copy_results(RESULTS_DIR, './results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RayTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray\n",
    "ray.init(memory=16 * 1024 * 1024 * 1024,\n",
    "         object_store_memory=8 * 1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = tune.utils.pin_in_object_store(train_loader), tune.utils.pin_in_object_store(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_other = tune.utils.pin_in_object_store(idx_of_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ray.remote\n",
    "def train_TW(config):\n",
    "    top_n_pred = [1, 3, 5]\n",
    "    model = learn.TWModel(input_size, config['hidden_size'], X_types_param.shape[1],\n",
    "                          config['num_layers'], output_size, True).to(device)\n",
    "\n",
    "    #for i in range(1, n_rep+1):\n",
    "    i=1\n",
    "\n",
    "    learn.train_loop_TW(model, config['train_loader'], config['learning_rate'], config['epochs'])\n",
    "    y_true, y_pred = learn.evaluate_TW(model, config['test_loader'], top_n=max(top_n_pred))\n",
    "#     learn.train_loop_TW(model, train_loader, config['learning_rate'], config['epochs'])\n",
    "#     y_true, y_pred = learn.evaluate_TW(model, test_loader, top_n=max(top_n_pred))\n",
    "\n",
    "    # Ignore other type\n",
    "    #idx_of_other = pickle.load(open(f'./output/ml_inputs/label_encoder.pkl', 'rb')).transform(['other'])[0]\n",
    "    idx = (y_true != tune.utils.get_pinned_object(idx_of_other)) & (y_pred[:, 0] != tune.utils.get_pinned_object(idx_of_other))\n",
    "    f1_score_top_n = []\n",
    "    for top_n in top_n_pred:\n",
    "        filename = f\"{learn.TWModel.__name__}_complete_{i}_{top_n}\"\n",
    "        #learn.report_TW(y_true, y_pred, top_n, f\"{filename}_unfiltered\")\n",
    "        report = learn.report_TW(y_true[idx], y_pred[idx], top_n, y_true.shape[0], f\"{filename}_filtered\")\n",
    "        f1_score_top_n.append(report['weighted avg']['f1-score'])\n",
    "    print(\"Mean f1_score:\", mean(f1_score_top_n))\n",
    "    ray.tune.track.log(mean_f1_score=mean(f1_score_top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(train_TW,\n",
    "           config={'hidden_size': tune.grid_search([32, 64, 128]),\n",
    "                   'num_layers': tune.grid_search([1]),\n",
    "                   'learning_rate': tune.grid_search([0.002]),\n",
    "                   'epochs': tune.grid_search([5]),\n",
    "                   'train_loader': train_loader,\n",
    "                   'test_loader': test_loader},\n",
    "                   name=\"TypeWriter_model\",\n",
    "                   resources_per_trial={\"cpu\": 2,\n",
    "                                        \"gpu\": 2},\n",
    "                   verbose=1)\n",
    "print(\"Best config: \", analysis.get_best_config(metric=\"mean_f1_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "91129dd2ecdbb560cfb2a023659818934bf4821c9639c127637a51720b50a62d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
